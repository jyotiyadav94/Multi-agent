{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installing Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama run llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (2024.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Using cached fsspec-2024.3.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl (292 kB)\n",
      "Using cached safetensors-0.4.2-cp312-cp312-macosx_11_0_arm64.whl (393 kB)\n",
      "Using cached tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached fsspec-2024.3.0-py3-none-any.whl (171 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.2\n",
      "    Uninstalling safetensors-0.4.2:\n",
      "      Successfully uninstalled safetensors-0.4.2\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.12.25\n",
      "    Uninstalling regex-2023.12.25:\n",
      "      Successfully uninstalled regex-2023.12.25\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.0\n",
      "    Uninstalling fsspec-2024.3.0:\n",
      "      Successfully uninstalled fsspec-2024.3.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.21.4\n",
      "    Uninstalling huggingface-hub-0.21.4:\n",
      "      Successfully uninstalled huggingface-hub-0.21.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-legacy 0.9.48 requires openai>=1.1.0, but you have openai 0.28.1 which is incompatible.\n",
      "llama-index-core 0.10.20.post2 requires openai>=1.1.0, but you have openai 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.3.0 huggingface-hub-0.21.4 idna-3.6 numpy-1.26.4 packaging-24.0 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.2 typing-extensions-4.10.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install -q transformers\n",
    "!pip -q install sentence-transformers\n",
    "!pip install --force-reinstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following yanked versions: 0.4.4, 0.4.4.post1\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 0.10.0 Requires-Python >=3.8.1,<3.12; 0.10.1 Requires-Python >=3.8.1,<3.12; 0.10.11 Requires-Python >=3.8.1,<3.12; 0.10.12 Requires-Python >=3.8.1,<3.12; 0.10.3 Requires-Python >=3.8.1,<3.12; 0.10.4 Requires-Python >=3.8.1,<3.12; 0.8.43 Requires-Python >=3.8.1,<3.12; 0.8.43.post1 Requires-Python >=3.8.1,<3.12; 0.8.44 Requires-Python >=3.8.1,<3.12; 0.8.45 Requires-Python >=3.8.1,<3.12; 0.8.45.post1 Requires-Python >=3.8.1,<3.12; 0.8.46 Requires-Python >=3.8.1,<3.12; 0.8.47 Requires-Python >=3.8.1,<3.12; 0.8.48 Requires-Python >=3.8.1,<3.12; 0.8.49 Requires-Python >=3.8.1,<3.12; 0.8.50 Requires-Python >=3.8.1,<3.12; 0.8.51 Requires-Python >=3.8.1,<3.12; 0.8.51.post1 Requires-Python >=3.8.1,<3.12; 0.8.52 Requires-Python >=3.8.1,<3.12; 0.8.53 Requires-Python >=3.8.1,<3.12; 0.8.53.post3 Requires-Python >=3.8.1,<3.12; 0.8.54 Requires-Python >=3.8.1,<3.12; 0.8.55 Requires-Python >=3.8.1,<3.12; 0.8.56 Requires-Python >=3.8.1,<3.12; 0.8.57 Requires-Python >=3.8.1,<3.12; 0.8.58 Requires-Python >=3.8.1,<3.12; 0.8.59 Requires-Python >=3.8.1,<3.12; 0.8.61 Requires-Python >=3.8.1,<3.12; 0.8.62 Requires-Python >=3.8.1,<3.12; 0.8.63.post1 Requires-Python >=3.8.1,<3.12; 0.8.63.post2 Requires-Python >=3.8.1,<3.12; 0.8.64 Requires-Python >=3.8.1,<3.12; 0.8.64.post1 Requires-Python >=3.8.1,<3.12; 0.8.65 Requires-Python >=3.8.1,<3.12; 0.8.66 Requires-Python >=3.8.1,<3.12; 0.8.67 Requires-Python >=3.8.1,<3.12; 0.8.68 Requires-Python >=3.8.1,<3.12; 0.8.69 Requires-Python >=3.8.1,<3.12; 0.8.69.post1 Requires-Python >=3.8.1,<3.12; 0.8.69.post2 Requires-Python >=3.8.1,<3.12; 0.9.0 Requires-Python >=3.8.1,<3.12; 0.9.0.post1 Requires-Python >=3.8.1,<3.12; 0.9.0a1 Requires-Python >=3.8.1,<3.12; 0.9.0a2 Requires-Python >=3.8.1,<3.12; 0.9.0a3 Requires-Python >=3.8.1,<3.12; 0.9.1 Requires-Python >=3.8.1,<3.12; 0.9.10 Requires-Python >=3.8.1,<3.12; 0.9.10a1 Requires-Python >=3.8.1,<3.12; 0.9.10a2 Requires-Python >=3.8.1,<3.12; 0.9.11 Requires-Python >=3.8.1,<3.12; 0.9.11.post1 Requires-Python >=3.8.1,<3.12; 0.9.2 Requires-Python >=3.8.1,<3.12; 0.9.3 Requires-Python >=3.8.1,<3.12; 0.9.3.post1 Requires-Python >=3.8.1,<3.12; 0.9.4 Requires-Python >=3.8.1,<3.12; 0.9.5 Requires-Python >=3.8.1,<3.12; 0.9.6 Requires-Python >=3.8.1,<3.12; 0.9.6.post1 Requires-Python >=3.8.1,<3.12; 0.9.6.post2 Requires-Python >=3.8.1,<3.12; 0.9.7 Requires-Python >=3.8.1,<3.12; 0.9.8 Requires-Python >=3.8.1,<3.12; 0.9.8.post1 Requires-Python >=3.8.1,<3.12; 0.9.9 Requires-Python >=3.8.1,<3.12\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index==0.8.59 (from versions: 0.4.4.post2, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13, 0.4.14, 0.4.15, 0.4.16, 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.22.post1, 0.4.23, 0.4.24, 0.4.25, 0.4.26, 0.4.27, 0.4.28, 0.4.29, 0.4.30, 0.4.31, 0.4.32, 0.4.33, 0.4.34, 0.4.35, 0.4.35.post1, 0.4.36, 0.4.37, 0.4.38, 0.4.39, 0.4.40, 0.5.0, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.5.5, 0.5.6, 0.5.7, 0.5.8, 0.5.9, 0.5.10, 0.5.11, 0.5.12, 0.5.13, 0.5.13.post1, 0.5.15, 0.5.16, 0.5.17, 0.5.17.post1, 0.5.18, 0.5.19, 0.5.20, 0.5.21, 0.5.22, 0.5.23, 0.5.23.post1, 0.5.25, 0.5.26, 0.5.27, 0.6.0a1, 0.6.0a2, 0.6.0a3, 0.6.0a4, 0.6.0a5, 0.6.0a6, 0.6.0a7, 0.6.0, 0.6.1, 0.6.2, 0.6.4, 0.6.5, 0.6.6, 0.6.7, 0.6.8, 0.6.9, 0.6.10, 0.6.10.post1, 0.6.11, 0.6.12, 0.6.13, 0.6.14, 0.6.15, 0.6.16, 0.6.16.post1, 0.6.17, 0.6.18, 0.6.19, 0.6.20, 0.6.21.post1, 0.6.22, 0.6.23, 0.6.24, 0.6.25, 0.6.25.post1, 0.6.26, 0.6.27, 0.6.28, 0.6.29, 0.6.30, 0.6.31, 0.6.32, 0.6.33, 0.6.34, 0.6.34.post1, 0.6.35, 0.6.36, 0.6.37, 0.6.38, 0.6.38.post1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.10.post1, 0.7.11, 0.7.11.post1, 0.7.12, 0.7.13, 0.7.14, 0.7.15, 0.7.16, 0.7.17, 0.7.18, 0.7.19, 0.7.20, 0.7.21, 0.7.22, 0.7.23, 0.7.24.post1, 0.8.0, 0.8.1, 0.8.1.post1, 0.8.2, 0.8.2.post1, 0.8.3, 0.8.4, 0.8.5, 0.8.5.post1, 0.8.5.post2, 0.8.6, 0.8.7, 0.8.8, 0.8.9, 0.8.10, 0.8.10.post1, 0.8.11, 0.8.11.post1, 0.8.11.post2, 0.8.11.post3, 0.8.12, 0.8.13, 0.8.14, 0.8.15, 0.8.16, 0.8.17, 0.8.18, 0.8.19, 0.8.20, 0.8.21, 0.8.22, 0.8.23, 0.8.23.post1, 0.8.24, 0.8.24.post1, 0.8.25, 0.8.26, 0.8.26.post1, 0.8.27, 0.8.28a1, 0.8.28, 0.8.29, 0.8.29.post1, 0.8.30, 0.8.31, 0.8.32, 0.8.33, 0.8.34, 0.8.35, 0.8.36, 0.8.37, 0.8.38, 0.8.39, 0.8.39.post2, 0.8.40, 0.8.41, 0.8.42, 0.9.12a1, 0.9.12a2, 0.9.12a3, 0.9.12a4, 0.9.12a5, 0.9.12a6, 0.9.12, 0.9.13, 0.9.14, 0.9.14.post1, 0.9.14.post2, 0.9.14.post3, 0.9.15, 0.9.15.post1, 0.9.15.post2, 0.9.16.dev1, 0.9.16.dev2, 0.9.16, 0.9.16.post1, 0.9.17.dev1, 0.9.17, 0.9.18, 0.9.19, 0.9.20, 0.9.21, 0.9.22, 0.9.23, 0.9.24, 0.9.25a1, 0.9.25a2, 0.9.25, 0.9.25.post1, 0.9.26, 0.9.27, 0.9.28, 0.9.28.post1, 0.9.28.post2, 0.9.29, 0.9.30, 0.9.31, 0.9.32, 0.9.33a2, 0.9.33a3, 0.9.33a4, 0.9.33a5, 0.9.33a6, 0.9.33, 0.9.34, 0.9.35, 0.9.36, 0.9.37, 0.9.37.post1, 0.9.38, 0.9.39, 0.9.40, 0.9.41, 0.9.42, 0.9.42.post1, 0.9.42.post2, 0.9.43, 0.9.44, 0.9.45, 0.9.45.post1, 0.9.46, 0.9.47, 0.9.48, 0.10.5a1, 0.10.5, 0.10.6, 0.10.7, 0.10.8, 0.10.9, 0.10.10, 0.10.13, 0.10.13.post1, 0.10.14, 0.10.15, 0.10.16, 0.10.17, 0.10.18, 0.10.19, 0.10.20)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index==0.8.59\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: openai==0.28.1 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from openai==0.28.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from openai==0.28.1) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from openai==0.28.1) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.1) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->openai==0.28.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->openai==0.28.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->openai==0.28.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->openai==0.28.1) (1.9.4)\n",
      "Requirement already satisfied: pypdf==3.17.2 in /Users/wenda/anaconda3/envs/myenv/lib/python3.12/site-packages (3.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install llama-index==0.8.59\n",
    "!pip3 install openai==0.28.1\n",
    "!pip3 install pypdf==3.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic questions without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Ollama' from 'llama_index.llms' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[1;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m resp \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcomplete(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat did Rome grow? Be concise.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Ollama' from 'llama_index.llms' (unknown location)"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama\n",
    "llm = Ollama(model=\"mistral\")\n",
    "resp = llm.complete(\"What did Rome grow? Be concise.\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #2 — Simple Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceContext\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleChatEngine\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.chat_engine import SimpleChatEngine\n",
    "#from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\", \n",
    ")\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(service_context=service_context)\n",
    "print(chat_engine.chat(\"Hi, my name is Mirna\"))\n",
    "#assistant: \n",
    "#Hi Mirna! It's nice to meet you. What can I assist you with today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "HuggingFaceEmbedding requires transformers to be installed.\nPlease install transformers with `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/embeddings/huggingface.py:61\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[0;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 24\u001b[0m\n\u001b[1;32m     13\u001b[0m documents \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     SimpleDirectoryReader(\n\u001b[1;32m     15\u001b[0m         input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/wenda/Documents/GITHUB/Artificial Intelligence BUFFET/Implementing Llama\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m         required_exts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ServiceContext is a bundle of commonly used \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# resources used during the indexing and \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# querying stage \u001b[39;00m\n\u001b[1;32m     23\u001b[0m service_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mServiceContext\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal:BAAI/bge-small-en-v1.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m set_global_service_context(service_context)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Node represents a “chunk” of a source Document\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/indices/service_context.py:168\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    165\u001b[0m         llm_predictor\u001b[38;5;241m.\u001b[39mquery_wrapper_prompt \u001b[38;5;241m=\u001b[39m query_wrapper_prompt\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[1;32m    171\u001b[0m prompt_helper \u001b[38;5;241m=\u001b[39m prompt_helper \u001b[38;5;129;01mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[1;32m    172\u001b[0m     llm_metadata\u001b[38;5;241m=\u001b[39mllm_predictor\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    173\u001b[0m     context_window\u001b[38;5;241m=\u001b[39mcontext_window,\n\u001b[1;32m    174\u001b[0m     num_output\u001b[38;5;241m=\u001b[39mnum_output,\n\u001b[1;32m    175\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/embeddings/utils.py:70\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model)\u001b[0m\n\u001b[1;32m     66\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m InstructorEmbedding(\n\u001b[1;32m     67\u001b[0m             model_name\u001b[38;5;241m=\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder\n\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, LCEmbeddings):\n\u001b[1;32m     75\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m LangchainEmbedding(embed_model)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/embeddings/huggingface.py:63\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[0;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceEmbedding requires transformers to be installed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install transformers with `pip install transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m infer_torch_device()\n\u001b[1;32m     70\u001b[0m cache_folder \u001b[38;5;241m=\u001b[39m cache_folder \u001b[38;5;129;01mor\u001b[39;00m get_cache_dir()\n",
      "\u001b[0;31mImportError\u001b[0m: HuggingFaceEmbedding requires transformers to be installed.\nPlease install transformers with `pip install transformers`."
     ]
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    set_global_service_context,\n",
    ")\n",
    "from llama_index.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Reads pdfs at \"./\" path\n",
    "documents = (\n",
    "    SimpleDirectoryReader(\n",
    "        input_dir = '/Users/wenda/Documents/GITHUB/Artificial Intelligence BUFFET/Implementing Llama',\n",
    "        required_exts = [\".pdf\"])\n",
    "        .load_data()\n",
    ")\n",
    "\n",
    "# ServiceContext is a bundle of commonly used \n",
    "# resources used during the indexing and \n",
    "# querying stage \n",
    "service_context = (\n",
    "    ServiceContext\n",
    "    .from_defaults(\n",
    "        llm=llm, \n",
    "        embed_model=\"local:BAAI/bge-small-en-v1.5\", \n",
    "        chunk_size=300\n",
    "    )\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "# Node represents a “chunk” of a source Document\n",
    "nodes = (\n",
    "    service_context\n",
    "    .node_parser\n",
    "    .get_nodes_from_documents(documents)\n",
    ")\n",
    "\n",
    "# offers core abstractions around storage of Nodes, \n",
    "# indices, and vectors\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "# Create the vectorstore index\n",
    "index = (\n",
    "    VectorStoreIndex\n",
    "    .from_documents(\n",
    "        documents, \n",
    "        storage_context=storage_context, \n",
    "        llm=llm\n",
    "        )\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the index\n",
    "query=\"\"\"What was the role of Quintus Fabius Pictor \n",
    "at take the Second Punic War?\"\"\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
